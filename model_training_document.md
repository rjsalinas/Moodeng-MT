# üöÄ Filipino-to-English Translation Model Training Documentation

## üìã Overview

This document provides comprehensive information about the current iteration of the Filipino-to-English translation model training system and the improvements made from the previous iteration. The system uses mBART-50 as the base model with advanced fine-tuning techniques.

## üîÑ Model Training Iterations

### **Previous Iteration (v1.0)**
- **Base Model**: facebook/mbart-large-50-many-to-many-mmt
- **Fine-tuning Method**: Basic PEFT/LoRA with minimal configuration
- **Training Duration**: Limited epochs (typically 3-5)
- **Loss Functions**: Standard Cross-Entropy only
- **Data Processing**: Basic text cleaning without Filipino-specific optimizations
- **Evaluation**: Simple validation loss tracking
- **Model Size**: ~66MB (adapter weights only)

**Limitations of Previous Iteration:**
- Limited training epochs led to underfitting
- Single loss function reduced model robustness
- No curriculum learning approach
- Basic data preprocessing without language-specific optimizations
- No advanced regularization techniques
- Limited evaluation metrics (only validation loss)

### **Current Iteration (v2.0) - Enhanced Advanced Training**

#### **üöÄ Major Improvements**

##### **1. Extended Training Architecture**
- **Training Epochs**: Increased from 3-5 to **10 epochs**
- **Curriculum Learning**: Implemented **4-phase progressive training**
  - Phase 1 (Epochs 1-3): Simple sentences with standard CE loss
  - Phase 2 (Epochs 4-6): Medium complexity with label smoothing
  - Phase 3 (Epochs 7-8): Complex sentences with focal loss
  - Phase 4 (Epochs 9-10): Mixed complexity with R-Drop regularization

##### **2. Advanced Loss Functions**
- **Multiple Loss Functions**: Dynamic selection based on training phase
  - **Cross-Entropy Loss**: Standard loss for simple sentences
  - **Label Smoothing Loss**: Better generalization for medium complexity
  - **Focal Loss**: Handles class imbalance and hard examples
  - **R-Drop Loss**: Advanced regularization for mixed complexity phase

##### **3. Enhanced LoRA Configuration**
- **Maximum Capacity**: Increased rank from basic to **r=64**
- **Comprehensive Targeting**: Extended target modules coverage
  - Attention components (q_proj, v_proj, k_proj, out_proj)
  - Feed-forward components (fc1, fc2)
  - Encoder and cross-attention layers
  - Normalization layers (layernorm_embedding, final_layer_norm)
  - Embedding layer (embed_tokens)
- **Optimized Parameters**: lora_alpha=128, lora_dropout=0.05

##### **4. Advanced Training Techniques**
- **Gradient Accumulation**: 4x effective batch size (4 √ó 4 = 16)
- **Enhanced Learning Rate Scheduling**: 
  - Longer warmup period (15% of total steps)
  - Cosine annealing with warmup
  - Optimized learning rate: 2e-4
- **Gradient Clipping**: Prevents exploding gradients (max_norm=1.0)
- **Mixed Precision Training**: Automatic mixed precision (AMP) support

##### **5. Filipino-Specific Data Processing**
- **Advanced Text Cleaning**:
  - Social media artifact removal (@mentions, #hashtags, RT)
  - URL removal and normalization
  - Filipino contraction handling (di'ba ‚Üí di ba, kasi'ng ‚Üí kasi ng)
  - Excessive punctuation cleanup
- **Data Quality Scoring**: Intelligent filtering based on:
  - Translation similarity thresholds
  - Proper sentence structure validation
  - Filipino content verification
- **Data Augmentation**: Synonym replacement for common Filipino words
- **Complexity-Based Sorting**: Curriculum learning preparation

##### **6. Enhanced Evaluation System**
- **BLEU Score Calculation**: Sentence-level BLEU with smoothing
- **Multiple Metrics**: Training loss, validation loss, and BLEU score
- **Enhanced Early Stopping**: Dual criteria (validation loss + BLEU score)
- **Patience Management**: Increased patience to 5 epochs

##### **7. Advanced Generation Parameters**
- **Beam Search**: Increased from basic to 5 beams
- **Sampling Techniques**: Top-k (k=50) and nucleus sampling (p=0.9)
- **Length Penalty**: Optimized for longer, more complete translations
- **Repetition Control**: No-repeat n-gram size of 3
- **Temperature Control**: 0.8 for controlled randomness

## üìä Technical Specifications

### **Model Architecture**
- **Base Model**: facebook/mbart-large-50-many-to-many-mmt
- **Fine-tuning Method**: PEFT (Parameter-Efficient Fine-Tuning)
- **Adapter Type**: LoRA (Low-Rank Adaptation)
- **Total Parameters**: ~610M (base model)
- **Trainable Parameters**: ~4.5M (0.74% of total)
- **Memory Efficiency**: 99.26% parameter reduction

### **Training Configuration**
- **Batch Size**: 4 (effective: 16 with accumulation)
- **Learning Rate**: 2e-4 with warmup + cosine annealing
- **Weight Decay**: 0.01
- **Optimizer**: AdamW with betas=(0.9, 0.999)
- **Scheduler**: Cosine with warmup (15% warmup steps)
- **Mixed Precision**: Automatic (AMP)

### **Data Processing**
- **Input Format**: CSV with src (Filipino) and tgt (English) columns
- **Text Length Limits**: 15-200 characters (source), 8-300 characters (target)
- **Quality Filtering**: Automatic data quality scoring
- **Augmentation**: Synonym replacement for common Filipino patterns
- **Complexity Calculation**: Word count + punctuation + mixed language detection

## üéØ Performance Improvements

### **Training Efficiency**
- **Extended Training**: 10 epochs vs 3-5 epochs (2-3x improvement)
- **Curriculum Learning**: Progressive complexity phases for better learning
- **Advanced Regularization**: R-Drop prevents overfitting in later phases
- **Gradient Accumulation**: Larger effective batch size without memory increase

### **Model Quality**
- **Multiple Loss Functions**: Better handling of different sentence complexities
- **Enhanced LoRA**: Maximum capacity utilization for better adaptation
- **Filipino-Specific Processing**: Language-aware data cleaning and augmentation
- **Advanced Evaluation**: BLEU score tracking for translation quality

### **Data Quality**
- **Intelligent Filtering**: Automatic removal of low-quality translation pairs
- **Language-Specific Cleaning**: Filipino contraction and pattern normalization
- **Augmentation**: Increased training data variety through synonym replacement
- **Complexity-Based Sorting**: Optimal training progression

## üìÅ File Structure

```
Moodeng-MT/
‚îú‚îÄ‚îÄ model_training.py                    # Enhanced training script (v2.0)
‚îú‚îÄ‚îÄ fine-tuned-mbart-tl2en/             # Current model output
‚îÇ   ‚îú‚îÄ‚îÄ adapter_config.json             # LoRA configuration
‚îÇ   ‚îú‚îÄ‚îÄ adapter_model.safetensors       # Trained adapter weights
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json                  # Tokenizer files
‚îÇ   ‚îî‚îÄ‚îÄ README.md                       # Model card
‚îú‚îÄ‚îÄ fine-tuned-mbart-tl2en-best/        # Best checkpoint (v2.0)
‚îÇ   ‚îú‚îÄ‚îÄ adapter_config.json             # Best LoRA configuration
‚îÇ   ‚îú‚îÄ‚îÄ adapter_model.safetensors       # Best adapter weights
‚îÇ   ‚îî‚îÄ‚îÄ README.md                       # Best model documentation
‚îî‚îÄ‚îÄ filipino_english_parallel_corpus.csv # Training dataset
```

## üöÄ Usage Instructions

### **Prerequisites**
```bash
pip install torch transformers peft pandas tqdm nltk
```

### **Training Execution**
```bash
python model_training.py
```

### **Key Features**
- **Automatic Dataset Detection**: Uses `filipino_english_parallel_corpus.csv` if available
- **Fallback Dataset**: Creates sample Filipino-English pairs if no dataset found
- **Progress Monitoring**: Real-time training progress with detailed metrics
- **Automatic Checkpointing**: Saves best model based on validation loss and BLEU score
- **Device Optimization**: Automatic CUDA detection with CPU fallback

## üìà Expected Outcomes

### **Training Performance**
- **Epoch 1-3**: Simple sentence mastery (basic Filipino greetings, simple structures)
- **Epoch 4-6**: Medium complexity handling (compound sentences, mixed language)
- **Epoch 7-8**: Complex sentence processing (longer texts, multiple clauses)
- **Epoch 9-10**: Mixed complexity refinement (R-Drop regularization)

### **Quality Metrics**
- **Training Loss**: Expected decrease from ~4.0 to ~1.5
- **Validation Loss**: Expected decrease from ~4.2 to ~1.8
- **BLEU Score**: Expected improvement from ~0.1 to ~0.4+
- **Translation Quality**: Significant improvement in Filipino-to-English accuracy

## üîß Customization Options

### **Training Parameters**
- **Epochs**: Adjustable (default: 10)
- **Batch Size**: Configurable (default: 4)
- **Learning Rate**: Tunable (default: 2e-4)
- **Curriculum Phases**: Customizable phase boundaries

### **Loss Function Selection**
- **Phase 1**: Standard Cross-Entropy
- **Phase 2**: Label Smoothing
- **Phase 3**: Focal Loss
- **Phase 4**: R-Drop

### **LoRA Configuration**
- **Rank (r)**: Adjustable from 16 to 64
- **Alpha**: Proportional to rank (typically 2√órank)
- **Target Modules**: Customizable module selection
- **Dropout**: Configurable (default: 0.05)

## üö® Troubleshooting

### **Common Issues**
1. **CUDA Out of Memory**: Reduce batch size or use gradient accumulation
2. **Training Instability**: Adjust learning rate or increase warmup steps
3. **Poor Convergence**: Check data quality or adjust curriculum phases
4. **BLEU Calculation Errors**: Ensure NLTK data is downloaded

### **Debug Mode**
```python
# Enable detailed logging in normalizer
normalizer = FilipinoNormalizer('rules.json', 'logs')
normalized, logs = normalizer.normalize_text(text, context={"debug": True})
```

## üìä Comparison Summary

| Aspect | Previous Iteration (v1.0) | Current Iteration (v2.0) | Improvement |
|--------|---------------------------|--------------------------|-------------|
| **Training Epochs** | 3-5 | 10 | 2-3x increase |
| **Loss Functions** | 1 (CE only) | 4 (CE, Label Smoothing, Focal, R-Drop) | 4x variety |
| **LoRA Rank** | Basic (16) | Maximum (64) | 4x capacity |
| **Curriculum Learning** | None | 4-phase progressive | New feature |
| **Data Processing** | Basic cleaning | Filipino-specific + augmentation | Enhanced quality |
| **Evaluation** | Validation loss only | Loss + BLEU + early stopping | Comprehensive |
| **Regularization** | Basic dropout | R-Drop + gradient clipping | Advanced |
| **Training Techniques** | Standard | Gradient accumulation + AMP | Optimized |

## üéØ Future Enhancements

### **Planned Improvements**
- **Multi-GPU Training**: Distributed training for larger datasets
- **Advanced Augmentation**: Back-translation and paraphrasing
- **Hyperparameter Optimization**: Bayesian optimization for best parameters
- **Model Compression**: Knowledge distillation for deployment
- **Real-time Translation**: Integration with preprocessing pipeline

### **Research Directions**
- **Cross-lingual Transfer**: Leveraging other Philippine languages
- **Domain Adaptation**: Specialized models for different text types
- **Quality Estimation**: Automatic translation quality assessment
- **Interactive Training**: Human-in-the-loop fine-tuning

## üìö References

- **mBART-50**: Liu et al. (2020) - Multilingual Denoising Pre-training
- **LoRA**: Hu et al. (2021) - Low-Rank Adaptation of Large Language Models
- **PEFT**: Mangrulkar et al. (2022) - Parameter-Efficient Fine-Tuning
- **R-Drop**: Liang et al. (2021) - Regularized Dropout for Neural Networks
- **Curriculum Learning**: Bengio et al. (2009) - Curriculum Learning

---

**üéâ The current iteration represents a significant advancement in Filipino-to-English translation model training, with comprehensive improvements across all aspects of the training pipeline!** üéâ

*Last Updated: Current Session*
*Version: 2.0 (Enhanced Advanced Training)*
